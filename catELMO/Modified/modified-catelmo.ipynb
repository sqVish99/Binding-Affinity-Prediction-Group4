{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:10:39.170663Z",
     "iopub.status.busy": "2024-12-14T03:10:39.170304Z",
     "iopub.status.idle": "2024-12-14T03:10:57.868964Z",
     "shell.execute_reply": "2024-12-14T03:10:57.868237Z",
     "shell.execute_reply.started": "2024-12-14T03:10:39.170634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# optional: To save/load files\n",
    "# dat.to_pickle('/kaggle/input/bert-embeddings-catelmo/epi_train_tiny_embeddings_768.pkl')\n",
    "# dat.to_pickle('/kaggle/input/bert-embeddings-catelmo/epi_train_tiny_embeddings_768.pkl')\n",
    "# dat = pd.read_pickle('/kaggle/input/bert-embeddings-catelmo/epi_train_tiny_embeddings_768.pkl')\n",
    "# dat2 = pd.read_pickle('/kaggle/input/bert-embeddings-catelmo/epi_test_tiny_embeddings_768.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:11:18.731975Z",
     "iopub.status.busy": "2024-12-14T03:11:18.731163Z",
     "iopub.status.idle": "2024-12-14T03:11:18.744574Z",
     "shell.execute_reply": "2024-12-14T03:11:18.743740Z",
     "shell.execute_reply.started": "2024-12-14T03:11:18.731940Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Tokenizer\n",
    "def tokenizer(sequence: str) -> List[str]:\n",
    "    sequence = re.sub(r'\\s+', '', str(sequence))\n",
    "    sequence = re.sub(r'[^ARNDCQEGHILKMFPSTWYVBZX]', '*', sequence)\n",
    "    return list(sequence)\n",
    "\n",
    "# Vocabulary mappings\n",
    "AMINO_MAP = {\n",
    "    '<pad>': 24, '*': 23, 'A': 0, 'C': 4, 'B': 20,\n",
    "    'E': 6, 'D': 3, 'G': 7, 'F': 13, 'I': 9, 'H': 8,\n",
    "    'K': 11, 'M': 12, 'L': 10, 'N': 2, 'Q': 5, 'P': 14,\n",
    "    'S': 15, 'R': 1, 'T': 16, 'W': 17, 'V': 19, 'Y': 18,\n",
    "    'X': 22, 'Z': 21\n",
    "}\n",
    "\n",
    "AMINO_MAP_REV = [\n",
    "    'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K',\n",
    "    'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'B', 'Z', 'X', '*', '@'\n",
    "]\n",
    "\n",
    "AMINO_MAP_REV_ = ['A','R','N','D','C','Q','E','G','H','I','L','K',\n",
    "                 'M','F','P','S','T','W','Y','V','N','Q','*','*','@']\n",
    "\n",
    "# Padding function\n",
    "def pad_sequence(sequence: List[int], max_length: int, pad_type: str = \"end\") -> List[int]:\n",
    "    pad_token = AMINO_MAP['<pad>']\n",
    "    if len(sequence) > max_length:\n",
    "        return sequence[:max_length]\n",
    "    padding = [pad_token] * (max_length - len(sequence))\n",
    "    if pad_type == \"front\":\n",
    "        return padding + sequence\n",
    "    elif pad_type == \"mid\":\n",
    "        half = len(padding) // 2\n",
    "        return padding[:half] + sequence + padding[half:]\n",
    "    else:\n",
    "        return sequence + padding\n",
    "\n",
    "\n",
    "\n",
    "def encode_sequence(sequence: List[str], max_length: int, pad_type: str) -> List[int]:\n",
    "        token_ids = [AMINO_MAP.get(token, AMINO_MAP['*']) for token in sequence]\n",
    "        return pad_sequence(token_ids, max_length, pad_type)\n",
    "\n",
    "\n",
    "def load_embedding(filename):\n",
    "    if filename is None or filename.lower() == 'none':\n",
    "        filename = '/kaggle/input/blosum/BLOSUM62.txt'\n",
    "    \n",
    "    embedding_file = open(filename, \"r\")\n",
    "    lines = embedding_file.readlines()[7:]\n",
    "    embedding_file.close()\n",
    "\n",
    "    embedding = [[float(x) for x in l.strip().split()[1:]] for l in lines]\n",
    "    embedding.append([0.0] * len(embedding[0]))\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:11:21.967908Z",
     "iopub.status.busy": "2024-12-14T03:11:21.967589Z",
     "iopub.status.idle": "2024-12-14T03:11:21.985154Z",
     "shell.execute_reply": "2024-12-14T03:11:21.984249Z",
     "shell.execute_reply.started": "2024-12-14T03:11:21.967883Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "embedding = load_embedding(None)\n",
    "num_amino = len(embedding)\n",
    "embedding_dim = len(embedding[0])\n",
    "nn_embedding = nn.Embedding(num_amino, embedding_dim, padding_idx=num_amino-1)\n",
    "embedding_model = nn_embedding.from_pretrained(torch.FloatTensor(embedding), freeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:11:23.750873Z",
     "iopub.status.busy": "2024-12-14T03:11:23.750170Z",
     "iopub.status.idle": "2024-12-14T03:11:23.757419Z",
     "shell.execute_reply": "2024-12-14T03:11:23.756501Z",
     "shell.execute_reply.started": "2024-12-14T03:11:23.750838Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "embedding_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:11:26.238879Z",
     "iopub.status.busy": "2024-12-14T03:11:26.238187Z",
     "iopub.status.idle": "2024-12-14T03:13:09.741073Z",
     "shell.execute_reply": "2024-12-14T03:13:09.740371Z",
     "shell.execute_reply.started": "2024-12-14T03:11:26.238844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "peptides = [encode_sequence(tokenizer(pep), 15, \"end\") for pep in dat['antigen'].tolist()]\n",
    "tcrs = [encode_sequence(tokenizer(dat['cdr3_sequence']), 25, \"end\")for tcr in dat['cdr3_sequence'].tolist()] \n",
    "\n",
    "peptides_test = [encode_sequence(tokenizer(pep), 15, \"end\") for pep in dat2['antigen'].tolist()]\n",
    "tcrs_test = [encode_sequence(tokenizer(dat['cdr3_sequence']), 25, \"end\")for tcr in dat2['cdr3_sequence'].tolist()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:14:43.598815Z",
     "iopub.status.busy": "2024-12-14T03:14:43.598418Z",
     "iopub.status.idle": "2024-12-14T03:15:26.389339Z",
     "shell.execute_reply": "2024-12-14T03:15:26.388590Z",
     "shell.execute_reply.started": "2024-12-14T03:14:43.598782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pep_blosum = embedding_model(torch.tensor(peptides).to(device))\n",
    "    tcr_blosum = embedding_model(torch.tensor(tcrs).to(device))\n",
    "    pep_blosum_test = embedding_model(torch.tensor(peptides_test).to(device))\n",
    "    tcr_blosum_test = embedding_model(torch.tensor(tcrs_test).to(device))\n",
    "pep_blosum = pep_blosum.tolist()\n",
    "tcr_blosum = tcr_blosum.tolist()\n",
    "pep_blosum_test = pep_blosum_test.tolist()\n",
    "tcr_blosum_test = tcr_blosum_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:14:31.644522Z",
     "iopub.status.busy": "2024-12-14T03:14:31.643678Z",
     "iopub.status.idle": "2024-12-14T03:14:31.650524Z",
     "shell.execute_reply": "2024-12-14T03:14:31.649592Z",
     "shell.execute_reply.started": "2024-12-14T03:14:31.644485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TCRDataset(Dataset):\n",
    "    def __init__(self, tcr_embeds, epi_embeds,pep_blosum, tcr_blosum, labels):\n",
    "        self.tcr_embeds = tcr_embeds\n",
    "        self.epi_embeds = epi_embeds\n",
    "        self.pep_blosum = pep_blosum\n",
    "        self.tcr_blosum = tcr_blosum\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.tcr_embeds[idx], dtype=torch.float32), \\\n",
    "               torch.tensor(self.epi_embeds[idx], dtype=torch.float32), \\\n",
    "               torch.tensor(self.pep_blosum[idx], dtype=torch.float32), \\\n",
    "               torch.tensor(self.tcr_blosum[idx], dtype=torch.float32), \\\n",
    "               torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:16:10.804075Z",
     "iopub.status.busy": "2024-12-14T03:16:10.803345Z",
     "iopub.status.idle": "2024-12-14T03:16:10.970447Z",
     "shell.execute_reply": "2024-12-14T03:16:10.969509Z",
     "shell.execute_reply.started": "2024-12-14T03:16:10.804036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = TCRDataset(dat['tcr_embeds'].tolist(),\n",
    "                           dat['epi_embeds'].tolist(),\n",
    "                           pep_blosum,\n",
    "                           tcr_blosum,\n",
    "                           dat['class'].tolist())\n",
    "\n",
    "test_dataset = TCRDataset(dat2['tcr_embeds'].tolist(),\n",
    "                          dat2['epi_embeds'].tolist(),\n",
    "                          pep_blosum_test,\n",
    "                          tcr_blosum_test,\n",
    "                          dat2['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:16:20.920948Z",
     "iopub.status.busy": "2024-12-14T03:16:20.920337Z",
     "iopub.status.idle": "2024-12-14T03:16:20.930348Z",
     "shell.execute_reply": "2024-12-14T03:16:20.929443Z",
     "shell.execute_reply.started": "2024-12-14T03:16:20.920911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BindingAffinityModel(nn.Module):\n",
    "    def __init__(self, size, blosum_embedding_dim, lstm_hidden_dim=128, ffn_hidden_dim=256):\n",
    "        super(BindingAffinityModel, self).__init__()\n",
    "        self.n_dim = size\n",
    "        \n",
    "        # LSTM branch for BLOSUM embeddings of TCR sequences\n",
    "        self.tcr_lstm = nn.LSTM(input_size=blosum_embedding_dim, \n",
    "                                hidden_size=lstm_hidden_dim, \n",
    "                                num_layers=1, \n",
    "                                batch_first=True, \n",
    "                                bidirectional=True)\n",
    "        \n",
    "        # LSTM branch for BLOSUM embeddings of epitope sequences\n",
    "        self.epitope_lstm = nn.LSTM(input_size=blosum_embedding_dim, \n",
    "                                    hidden_size=lstm_hidden_dim, \n",
    "                                    num_layers=1, \n",
    "                                    batch_first=True, \n",
    "                                    bidirectional=True)\n",
    "        \n",
    "        # Branch A (FFN for TCR embeddings)\n",
    "        self.branchA = nn.Sequential(\n",
    "            nn.Linear(self.n_dim, self.n_dim * 2),\n",
    "            nn.BatchNorm1d(self.n_dim * 2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # Branch B (FFN for epitope embeddings)\n",
    "        self.branchB = nn.Sequential(\n",
    "            nn.Linear(self.n_dim, self.n_dim * 2),\n",
    "            nn.BatchNorm1d(self.n_dim * 2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # Combined FFN layers\n",
    "        self.combined = nn.Sequential(\n",
    "            nn.Linear(self.n_dim * 2 * 2 + 4 * lstm_hidden_dim, self.n_dim),\n",
    "            nn.BatchNorm1d(self.n_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.n_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, tcr_blosum_seq, epitope_blosum_seq, inputA, inputB):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - tcr_blosum_seq: Tensor [batch_size, seq_len, blosum_embedding_dim] (BLOSUM TCR)\n",
    "        - epitope_blosum_seq: Tensor [batch_size, seq_len, blosum_embedding_dim] (BLOSUM Epitope)\n",
    "        - inputA: Tensor [batch_size, size] (TCR BERT Embedding)\n",
    "        - inputB: Tensor [batch_size, size] (Epitope BERT Embedding)\n",
    "        \"\"\"\n",
    "        # Process TCR BLOSUM sequence through LSTM\n",
    "        tcr_lstm_out, _ = self.tcr_lstm(tcr_blosum_seq)  # Shape: [batch_size, seq_len, 2*lstm_hidden_dim]\n",
    "        tcr_lstm_out = tcr_lstm_out[:, -1, :]  # Take last hidden state\n",
    "        \n",
    "        # Process Epitope BLOSUM sequence through LSTM\n",
    "        epitope_lstm_out, _ = self.epitope_lstm(epitope_blosum_seq)  # Shape: [batch_size, seq_len, 2*lstm_hidden_dim]\n",
    "        epitope_lstm_out = epitope_lstm_out[:, -1, :]  # Take last hidden state\n",
    "        \n",
    "        # Process BERT embeddings through FFN branches\n",
    "        x = self.branchA(inputA)  # Shape: [batch_size, n_dim * 2]\n",
    "        y = self.branchB(inputB)  # Shape: [batch_size, n_dim * 2]\n",
    "        \n",
    "        # Concatenate LSTM outputs with branch outputs\n",
    "        combined = torch.cat((x, y, tcr_lstm_out, epitope_lstm_out), dim=1)  # Shape: [batch_size, n_dim*4 + 4*lstm_hidden_dim]\n",
    "        \n",
    "        # Pass through combined FFN\n",
    "        z = self.combined(combined)  # Final output\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:16:26.736215Z",
     "iopub.status.busy": "2024-12-14T03:16:26.735897Z",
     "iopub.status.idle": "2024-12-14T03:21:23.511707Z",
     "shell.execute_reply": "2024-12-14T03:21:23.510783Z",
     "shell.execute_reply.started": "2024-12-14T03:16:26.736189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "FFnmodel = BindingAffinityModel(768, 24).to(device)\n",
    "optimizer = torch.optim.Adam(FFnmodel.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Data preparation (replace with actual data)\n",
    "# train_dataset = TCRDataset(tcr_embeds_train, epi_embeds_train, labels_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# test_dataset = TCRDataset(tcr_embeds_test, epi_embeds_test, labels_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    FFnmodel.train()\n",
    "    epoch_loss = 0\n",
    "    train_preds, train_labels = [], []\n",
    "    \n",
    "    for tcr, epi, tcr_blosum, epi_blosum, label in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        tcr, epi, label,tcr_blosum, epi_blosum = tcr.to(device), epi.to(device), label.to(device), tcr_blosum.to(device), epi_blosum.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = FFnmodel(tcr_blosum, epi_blosum,tcr, epi).squeeze()\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        train_preds.extend(outputs.detach().cpu().numpy())\n",
    "        train_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    # Metrics\n",
    "    train_preds_binary = [1 if p >= 0.5 else 0 for p in train_preds]\n",
    "    acc = accuracy_score(train_labels, train_preds_binary)\n",
    "    prec = precision_score(train_labels, train_preds_binary)\n",
    "    rec = recall_score(train_labels, train_preds_binary)\n",
    "    auc = roc_auc_score(train_labels, train_preds)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {epoch_loss/len(train_loader):.4f}, Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = 'modified_catelmo_epi_weights.pth'\n",
    "torch.save(FFnmodel.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T03:21:44.031683Z",
     "iopub.status.busy": "2024-12-14T03:21:44.030990Z",
     "iopub.status.idle": "2024-12-14T03:21:53.795790Z",
     "shell.execute_reply": "2024-12-14T03:21:53.795021Z",
     "shell.execute_reply.started": "2024-12-14T03:21:44.031641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FFnmodel.eval()\n",
    "test_preds, test_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for tcr, epi, tcr_blosum, epi_blosum, label in tqdm(test_loader, desc=\"Testing\"):\n",
    "        tcr, epi, label, tcr_blosum, epi_blosum = tcr.to(device), epi.to(device), label.to(device), tcr_blosum.to(device), epi_blosum.to(device)\n",
    "        outputs = FFnmodel(tcr_blosum, epi_blosum, tcr, epi).squeeze()\n",
    "        test_preds.extend(outputs.cpu().numpy())\n",
    "        test_labels.extend(label.cpu().numpy())\n",
    "\n",
    "# Test metrics\n",
    "test_preds_binary = [1 if p >= 0.5 else 0 for p in test_preds]\n",
    "acc = accuracy_score(test_labels, test_preds_binary)\n",
    "prec = precision_score(test_labels, test_preds_binary)\n",
    "rec = recall_score(test_labels, test_preds_binary)\n",
    "auc = roc_auc_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Test Metrics - Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'True_Labels': test_labels,\n",
    "    'Predicted_Scores': test_preds,\n",
    "    'Predicted_Binary': test_preds_binary\n",
    "})\n",
    "\n",
    "csv_filename = 'modified_epi_test_predictions.csv'\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Predictions and labels saved to: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def hyperparameter_tuning(dat, pep_blosum, tcr_blosum, \n",
    "                           param_grid={\n",
    "                               'learning_rate': [0.001, 0.0001],\n",
    "                               'batch_size': [32, 64],\n",
    "                               'hidden_dim': [64, 128],\n",
    "                               'dropout': [0.2, 0.3, 0.4]\n",
    "                           },\n",
    "                           n_splits=5, \n",
    "                           epochs=5):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning using grid search with cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    - dat: DataFrame containing dataset\n",
    "    - pep_blosum: Peptide BLOSUM embeddings\n",
    "    - tcr_blosum: TCR BLOSUM embeddings\n",
    "    - param_grid: Dictionary of hyperparameters to tune\n",
    "    - n_splits: Number of cross-validation splits\n",
    "    - epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "    - Best hyperparameters and corresponding performance metrics\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Generate all hyperparameter combinations\n",
    "    param_combinations = [\n",
    "        dict(zip(param_grid.keys(), v)) \n",
    "        for v in itertools.product(*param_grid.values())\n",
    "    ]\n",
    "    \n",
    "    # Store results for each hyperparameter combination\n",
    "    results = []\n",
    "    \n",
    "    for params in param_combinations:\n",
    "        print(f\"\\n--- Hyperparameter Configuration ---\")\n",
    "        for k, v in params.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "        \n",
    "        # Stratified K-Fold\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        \n",
    "        # Fold-wise performance metrics\n",
    "        fold_metrics = {\n",
    "            'accuracies': [],\n",
    "            'precisions': [],\n",
    "            'recalls': [],\n",
    "            'aucs': []\n",
    "        }\n",
    "        \n",
    "        labels = dat['class'].tolist()\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(skf.split(dat, labels), 1):\n",
    "            print(f\"\\nFold {fold}\")\n",
    "            \n",
    "            # Prepare datasets for this fold\n",
    "            train_df = dat.iloc[train_index]\n",
    "            val_df = dat.iloc[val_index]\n",
    "            \n",
    "            train_pep = [pep_blosum[i] for i in train_index]\n",
    "            val_pep = [pep_blosum[i] for i in val_index]\n",
    "            train_tcr = [tcr_blosum[i] for i in train_index]\n",
    "            val_tcr = [tcr_blosum[i] for i in val_index]\n",
    "            \n",
    "            # Create datasets\n",
    "            train_dataset = TCRDataset(\n",
    "                train_df['tcr_embeds'].tolist(),\n",
    "                train_df['epi_embeds'].tolist(),\n",
    "                train_pep,\n",
    "                train_tcr,\n",
    "                train_df['class'].tolist()\n",
    "            )\n",
    "            \n",
    "            val_dataset = TCRDataset(\n",
    "                val_df['tcr_embeds'].tolist(),\n",
    "                val_df['epi_embeds'].tolist(),\n",
    "                val_pep,\n",
    "                val_tcr,\n",
    "                val_df['class'].tolist()\n",
    "            )\n",
    "            \n",
    "            # DataLoaders\n",
    "            train_loader = DataLoader(train_dataset, \n",
    "                                      batch_size=params['batch_size'], \n",
    "                                      shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, \n",
    "                                    batch_size=params['batch_size'], \n",
    "                                    shuffle=False)\n",
    "            \n",
    "            # Modify model to include custom hidden dimension and dropout\n",
    "            class TunedBindingAffinityModel(nn.Module):\n",
    "                def __init__(self, size, blosum_embedding_dim, \n",
    "                             lstm_hidden_dim=128, \n",
    "                             ffn_hidden_dim=256, \n",
    "                             dropout_rate=0.3):\n",
    "                    super(TunedBindingAffinityModel, self).__init__()\n",
    "                    self.n_dim = size\n",
    "                    \n",
    "                    # LSTM branches (similar to original model)\n",
    "                    self.tcr_lstm = nn.LSTM(input_size=blosum_embedding_dim, \n",
    "                                            hidden_size=lstm_hidden_dim, \n",
    "                                            num_layers=1, \n",
    "                                            batch_first=True, \n",
    "                                            bidirectional=True)\n",
    "                    \n",
    "                    self.epitope_lstm = nn.LSTM(input_size=blosum_embedding_dim, \n",
    "                                                hidden_size=lstm_hidden_dim, \n",
    "                                                num_layers=1, \n",
    "                                                batch_first=True, \n",
    "                                                bidirectional=True)\n",
    "                    \n",
    "                    # Branch A with customized dropout\n",
    "                    self.branchA = nn.Sequential(\n",
    "                        nn.Linear(self.n_dim, self.n_dim * 2),\n",
    "                        nn.BatchNorm1d(self.n_dim * 2),\n",
    "                        nn.Dropout(dropout_rate),\n",
    "                        nn.SiLU()\n",
    "                    )\n",
    "                    \n",
    "                    # Branch B with customized dropout\n",
    "                    self.branchB = nn.Sequential(\n",
    "                        nn.Linear(self.n_dim, self.n_dim * 2),\n",
    "                        nn.BatchNorm1d(self.n_dim * 2),\n",
    "                        nn.Dropout(dropout_rate),\n",
    "                        nn.SiLU()\n",
    "                    )\n",
    "                    \n",
    "                    # Combined FFN layers with potential modifications\n",
    "                    self.combined = nn.Sequential(\n",
    "                        nn.Linear(self.n_dim * 2 * 2 + 4 * lstm_hidden_dim, self.n_dim),\n",
    "                        nn.BatchNorm1d(self.n_dim),\n",
    "                        nn.Dropout(dropout_rate),\n",
    "                        nn.SiLU(),\n",
    "                        nn.Linear(self.n_dim, 1),\n",
    "                        nn.Sigmoid()\n",
    "                    )\n",
    "                \n",
    "                def forward(self, tcr_blosum_seq, epitope_blosum_seq, inputA, inputB):\n",
    "                    # Same forward method as original model\n",
    "                    tcr_lstm_out, _ = self.tcr_lstm(tcr_blosum_seq)\n",
    "                    tcr_lstm_out = tcr_lstm_out[:, -1, :]\n",
    "                    \n",
    "                    epitope_lstm_out, _ = self.epitope_lstm(epitope_blosum_seq)\n",
    "                    epitope_lstm_out = epitope_lstm_out[:, -1, :]\n",
    "                    \n",
    "                    x = self.branchA(inputA)\n",
    "                    y = self.branchB(inputB)\n",
    "                    \n",
    "                    combined = torch.cat((x, y, tcr_lstm_out, epitope_lstm_out), dim=1)\n",
    "                    \n",
    "                    z = self.combined(combined)\n",
    "                    return z\n",
    "            \n",
    "            # Initialize model with tuned parameters\n",
    "            model = TunedBindingAffinityModel(\n",
    "                size=768, \n",
    "                blosum_embedding_dim=24, \n",
    "                lstm_hidden_dim=params['lstm_hidden_dim'],\n",
    "                dropout_rate=params['dropout']\n",
    "            ).to(device)\n",
    "            \n",
    "            # Optimizer with tuned learning rate\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "            criterion = nn.BCELoss()\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                val_preds, val_labels = [], []\n",
    "                \n",
    "                for tcr, epi, tcr_blosum, epi_blosum, label in train_loader:\n",
    "                    tcr = tcr.to(device)\n",
    "                    epi = epi.to(device)\n",
    "                    label = label.to(device)\n",
    "                    tcr_blosum = tcr_blosum.to(device)\n",
    "                    epi_blosum = epi_blosum.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(tcr_blosum, epi_blosum, tcr, epi).squeeze()\n",
    "                    loss = criterion(outputs, label)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for tcr, epi, tcr_blosum, epi_blosum, label in val_loader:\n",
    "                        tcr = tcr.to(device)\n",
    "                        epi = epi.to(device)\n",
    "                        label = label.to(device)\n",
    "                        tcr_blosum = tcr_blosum.to(device)\n",
    "                        epi_blosum = epi_blosum.to(device)\n",
    "                        \n",
    "                        outputs = model(tcr_blosum, epi_blosum, tcr, epi).squeeze()\n",
    "                        val_preds.extend(outputs.cpu().numpy())\n",
    "                        val_labels.extend(label.cpu().numpy())\n",
    "                \n",
    "                # Compute metrics for this fold\n",
    "                val_preds_binary = [1 if p >= 0.5 else 0 for p in val_preds]\n",
    "                fold_metrics['accuracies'].append(accuracy_score(val_labels, val_preds_binary))\n",
    "                fold_metrics['precisions'].append(precision_score(val_labels, val_preds_binary))\n",
    "                fold_metrics['recalls'].append(recall_score(val_labels, val_preds_binary))\n",
    "                fold_metrics['aucs'].append(roc_auc_score(val_labels, val_preds))\n",
    "        \n",
    "        # Compute average metrics for this hyperparameter configuration\n",
    "        avg_metrics = {\n",
    "            'mean_accuracy': np.mean(fold_metrics['accuracies']),\n",
    "            'std_accuracy': np.std(fold_metrics['accuracies']),\n",
    "            'mean_precision': np.mean(fold_metrics['precisions']),\n",
    "            'std_precision': np.std(fold_metrics['precisions']),\n",
    "            'mean_recall': np.mean(fold_metrics['recalls']),\n",
    "            'std_recall': np.std(fold_metrics['recalls']),\n",
    "            'mean_auc': np.mean(fold_metrics['aucs']),\n",
    "            'std_auc': np.std(fold_metrics['aucs']),\n",
    "            'params': params\n",
    "        }\n",
    "        \n",
    "        results.append(avg_metrics)\n",
    "        \n",
    "        print(\"\\nAverage Metrics:\")\n",
    "        print(f\"Accuracy: {avg_metrics['mean_accuracy']:.4f} ± {avg_metrics['std_accuracy']:.4f}\")\n",
    "        print(f\"Precision: {avg_metrics['mean_precision']:.4f} ± {avg_metrics['std_precision']:.4f}\")\n",
    "        print(f\"Recall: {avg_metrics['mean_recall']:.4f} ± {avg_metrics['std_recall']:.4f}\")\n",
    "        print(f\"AUC: {avg_metrics['mean_auc']:.4f} ± {avg_metrics['std_auc']:.4f}\")\n",
    "    \n",
    "    # Sort results by AUC in descending order\n",
    "    results.sort(key=lambda x: x['mean_auc'], reverse=True)\n",
    "    \n",
    "    print(\"\\n--- Best Hyperparameters ---\")\n",
    "    best_result = results[0]\n",
    "    for k, v in best_result['params'].items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    \n",
    "    print(\"\\nBest Performance Metrics:\")\n",
    "    print(f\"Accuracy: {best_result['mean_accuracy']:.4f} ± {best_result['std_accuracy']:.4f}\")\n",
    "    print(f\"Precision: {best_result['mean_precision']:.4f} ± {best_result['std_precision']:.4f}\")\n",
    "    print(f\"Recall: {best_result['mean_recall']:.4f} ± {best_result['std_recall']:.4f}\")\n",
    "    print(f\"AUC: {best_result['mean_auc']:.4f} ± {best_result['std_auc']:.4f}\")\n",
    "    \n",
    "    return results, best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_param_grid = {\n",
    "    'learning_rate': [0.001, 0.005, 0.000001],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'lstm_hidden_dim': [64, 128, 256],\n",
    "    'dropout': [0.2, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "results, best_params = hyperparameter_tuning(\n",
    "    dat, \n",
    "    pep_blosum, \n",
    "    tcr_blosum, \n",
    "    param_grid=custom_param_grid,\n",
    "    n_splits=5,\n",
    "    epochs=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6269395,
     "sourceId": 10154626,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6269805,
     "sourceId": 10155208,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6296017,
     "sourceId": 10190390,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6296753,
     "sourceId": 10191365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6298681,
     "sourceId": 10193955,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
